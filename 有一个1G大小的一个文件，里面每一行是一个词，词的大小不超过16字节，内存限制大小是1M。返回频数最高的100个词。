1 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。
这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，
直到分解得到的小文件的大小都不超过1M。

2 hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。

3 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。
最后就是把这5000个文件进行归并（merge sort类似于归并排序）的过程了。
